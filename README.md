# An Empirical Study of the Security Vulnerabilities of GPTs

<p align="center">
  <img src="https://img.shields.io/badge/Paper-Under_Review-blue">
  <img src="https://img.shields.io/badge/License-MIT-green">
  <img src="https://img.shields.io/badge/Dataset-Available-orange">
</p>

## ğŸ“– Introduction
This repository contains the **code, Attack Method Corpus and leaked information we obtained** accompanying our paper *"An Empirical Study of the Security Vulnerabilities of GPTs"* (currently under review).  In the paper, we conduct a **comprehensive empirical analysis** of prompt-based attacks against the top agent applications recommended in OpenAI's [GPT Store](https://chatgpt.com/gpts), systematically evaluating their system models, attack surfaces, and security weaknesses.


Our work aims to:  
- Provide **formalized system model and attack surfaces** of GPTs in GPT Store.
- Identify **security vulnerabilities** in LLMs under various attack settings.  
- Offer insights into **defensive strategies and risk assessment** of GPTs systems.  


## ğŸ§© Repository Structure
```text
â”œâ”€â”€ data for attack/                        
â”‚   â”œâ”€â”€ adversarial prompts/     # Attack prompts used in experiments
â”‚   â”œâ”€â”€ results/                 # Leaked information of top GPTs we obtained from basic attack
â”‚   â””â”€â”€ results from variants/   # Leaked information of top GPTs we obtained from Attack Method Corpus
â”‚
â”œâ”€â”€ data for defenes/                        
â”‚   â”œâ”€â”€ defensive prompts/           # Defensive prompts used in experiments
â”‚   â”œâ”€â”€ reverse-engineering GPTs/    # Leaked information of top GPTs we obtained
â”‚   â””â”€â”€ metadata.json                # Dataset description
â”‚
â”œâ”€â”€ src/                 # Core implementation
â”‚   â”œâ”€â”€ attacks/         # Attack algorithms and generation scripts
â”‚   â”œâ”€â”€ evaluation/      # Evaluation metrics and robustness tests
â”‚   â””â”€â”€ utils/           # Helper functions
â”‚
â”œâ”€â”€ requirements.txt     # Python dependencies
â””â”€â”€ README.md            # Documentation
```

---

## ğŸš€ Quick Start

### 1. Installation

```bash
git clone https://github.com/your-username/gpt-security-vulnerabilities.git

pip install -r requirements.txt
```

### 2. Running Attacks
```bash
python src/main.py \
  --input gpts.csv \          # List of GPTs to be attack
  --output out \              # Output files
  --quesion "..." \           # Attack prompts
  --head --reuse-profile      
```



## âš ï¸ Ethical Considerations

This repository is released **for academic research purposes only**. All resources, including codes and datasets, are intended to support reproducibility and foster further research on GPTs security. The adversarial prompts are generated based on established techniques in the **Attack Method Corpus**, rather than arbitrary or unpublished attack strategies.  

- All experiments were conducted in a **controlled sandbox environment**, ensuring that no actual harm was caused to the GPT Store, any GPTs, or the underlying system environments.
- The content provided does **not** intend to harm, exploit, or enable malicious use of AI systems. Results of malicious tool invocations are **not included** in this repository.  
- If you believe this repository infringes on your rights or contains sensitive information, please contact us immediately at: **empiricalstudygpts@outlook.com**. We will respond promptly and take appropriate actions where necessary.  
